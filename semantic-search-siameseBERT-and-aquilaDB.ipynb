{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "liIk8f980xT5"
   },
   "source": [
    "# Sentence Embeddings using Siamese BERT-Networks & Aquila\n",
    "---\n",
    "This Google Colab Notebook illustrates using the Sentence Transformer python library to quickly create BERT embeddings for sentences, storeing them in Aquila to perform fast semantic searches.\n",
    "\n",
    "https://github.com/aneesha/SiameseBERT-Notebook/blob/master/SiameseBERT_SemanticSearch.ipynb\n",
    "The Sentence Transformer library is available on [pypi](https://pypi.org/project/sentence-transformers/) and [github](https://github.com/UKPLab/sentence-transformers). The library implements code from the ACL 2019 paper entitled \"[Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://www.aclweb.org/anthology/D19-1410.pdf)\" by Nils Reimers and Iryna Gurevych. \n",
    "Embeddings are stored in https://aquiladb.xyz/docs/introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bzljhyTQEZds"
   },
   "source": [
    "## Install  Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 768
    },
    "colab_type": "code",
    "id": "AmxRYxNDvn6y",
    "outputId": "81694439-a0c9-45f1-88e6-ea77aa94f39a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/site-packages (0.2.5)\n",
      "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.7/site-packages (from sentence-transformers) (1.3.0.post2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/site-packages (from sentence-transformers) (1.16.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/site-packages (from sentence-transformers) (1.3.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/site-packages (from sentence-transformers) (0.22.2.post1)\n",
      "Requirement already satisfied: transformers==2.3.0 in /usr/local/lib/python3.7/site-packages (from sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.7/site-packages (from sentence-transformers) (3.4.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/site-packages (from sentence-transformers) (4.36.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (0.14.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/site-packages (from transformers==2.3.0->sentence-transformers) (0.1.83)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/site-packages (from transformers==2.3.0->sentence-transformers) (2019.8.19)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/site-packages (from transformers==2.3.0->sentence-transformers) (2.20.1)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/site-packages (from transformers==2.3.0->sentence-transformers) (1.9.242)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/site-packages (from transformers==2.3.0->sentence-transformers) (0.0.35)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from nltk->sentence-transformers) (1.12.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests->transformers==2.3.0->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /Users/randyhoulahan/Library/Python/3.7/lib/python/site-packages (from requests->transformers==2.3.0->sentence-transformers) (1.24.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests->transformers==2.3.0->sentence-transformers) (2019.3.9)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests->transformers==2.3.0->sentence-transformers) (2.7)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /Users/randyhoulahan/Library/Python/3.7/lib/python/site-packages (from boto3->transformers==2.3.0->sentence-transformers) (0.2.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /Users/randyhoulahan/Library/Python/3.7/lib/python/site-packages (from boto3->transformers==2.3.0->sentence-transformers) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.242 in /usr/local/lib/python3.7/site-packages (from boto3->transformers==2.3.0->sentence-transformers) (1.12.242)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/site-packages (from sacremoses->transformers==2.3.0->sentence-transformers) (7.0)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /Users/randyhoulahan/Library/Python/3.7/lib/python/site-packages (from botocore<1.13.0,>=1.12.242->boto3->transformers==2.3.0->sentence-transformers) (0.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.242->boto3->transformers==2.3.0->sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: aquiladb in /usr/local/lib/python3.7/site-packages (0.5.1)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/site-packages (from aquiladb) (3.8.0)\n",
      "Requirement already satisfied: grpcio in /usr/local/lib/python3.7/site-packages (from aquiladb) (1.21.1)\n",
      "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/site-packages (from protobuf->aquiladb) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from protobuf->aquiladb) (40.8.0)\n"
     ]
    }
   ],
   "source": [
    "# Install the library using pip\n",
    "!pip3 install sentence-transformers\n",
    "!pip3 install aquiladb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eIAKz6KVEndZ"
   },
   "source": [
    "## Load the BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "5IO_j2Ofv5pq",
    "outputId": "717b9097-4c19-49b0-97cb-b8c31c84d3ec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0310 13:41:22.045379 4592192960 file_utils.py:35] PyTorch version 1.3.0.post2 available.\n",
      "I0310 13:41:24.625150 4592192960 SentenceTransformer.py:29] Load pretrained SentenceTransformer: bert-base-nli-mean-tokens\n",
      "I0310 13:41:24.625917 4592192960 SentenceTransformer.py:32] Did not find a / or \\ in the name. Assume to download model from server\n",
      "I0310 13:41:24.627104 4592192960 SentenceTransformer.py:68] Load SentenceTransformer from folder: /Users/randyhoulahan/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip\n",
      "I0310 13:41:24.640283 4592192960 configuration_utils.py:182] loading configuration file /Users/randyhoulahan/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/config.json\n",
      "I0310 13:41:24.641008 4592192960 configuration_utils.py:199] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0310 13:41:24.642117 4592192960 modeling_utils.py:403] loading weights file /Users/randyhoulahan/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/pytorch_model.bin\n",
      "I0310 13:41:26.675991 4592192960 tokenization_utils.py:327] Model name '/Users/randyhoulahan/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/Users/randyhoulahan/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT' is a path or url to a directory containing tokenizer files.\n",
      "I0310 13:41:26.677552 4592192960 tokenization_utils.py:359] Didn't find file /Users/randyhoulahan/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/tokenizer_config.json. We won't load it.\n",
      "I0310 13:41:26.678401 4592192960 tokenization_utils.py:395] loading file /Users/randyhoulahan/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/vocab.txt\n",
      "I0310 13:41:26.678948 4592192960 tokenization_utils.py:395] loading file /Users/randyhoulahan/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/added_tokens.json\n",
      "I0310 13:41:26.679635 4592192960 tokenization_utils.py:395] loading file /Users/randyhoulahan/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/special_tokens_map.json\n",
      "I0310 13:41:26.680178 4592192960 tokenization_utils.py:395] loading file None\n",
      "I0310 13:41:26.714053 4592192960 SentenceTransformer.py:89] Use pytorch device: cpu\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "# run local# => docker run -d -i -p 50051:50051 -v \"data:/data\" -t ammaorg/aquiladb:latest\n",
    "from aquiladb import AquilaClient as acl\n",
    "db = acl('localhost', 50051)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I7_Ib3ITEwgO"
   },
   "source": [
    "## Setup a Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "rEa50nefv7HY",
    "outputId": "be7fcea8-15ac-4ab6-ebdf-55e507a7d496"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 12.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample BERT embedding vector - length 768\n",
      "status: true\n",
      "_id: \"8978081260d8d62dcadf0bff61351c56\"\n",
      "\n",
      "status: true\n",
      "_id: \"940606563a91786587b0cb04e9b729f6\"\n",
      "\n",
      "status: true\n",
      "_id: \"90d47bb88195e982ee99f6982380f1b4\"\n",
      "\n",
      "status: true\n",
      "_id: \"51e5fb9f875637da1a933660495ad48c\"\n",
      "\n",
      "status: true\n",
      "_id: \"cee5db6383e0a6deb46102b89582cefa\"\n",
      "\n",
      "status: true\n",
      "_id: \"2d552cb4d4a8df629bb83d0651d2cdb8\"\n",
      "\n",
      "status: true\n",
      "_id: \"efd614d03c4922977f0461ec9e63e051\"\n",
      "\n",
      "status: true\n",
      "_id: \"5df9f748c1e6ea266f850708d3d4c9d0\"\n",
      "\n",
      "status: true\n",
      "_id: \"2fd906c0d42506f247ed2efa2fc0181f\"\n",
      "\n",
      "status: true\n",
      "_id: \"6454f7e82b4522b6f9ffe2b488d48981\"\n",
      "\n",
      "status: true\n",
      "_id: \"a208dae0f07cb623ddf6426f7470ff03\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A corpus is a list with documents split by sentences.\n",
    "\n",
    "sentences = ['Absence of sanity', \n",
    "             'Lack of saneness',\n",
    "             'A man is eating food.',\n",
    "             'A man is eating a piece of bread.',\n",
    "             'The girl is carrying a baby.',\n",
    "             'A man is riding a horse.',\n",
    "             'A woman is playing violin.',\n",
    "             'Two men pushed carts through the woods.',\n",
    "             'A man is riding a white horse on an enclosed ground.',\n",
    "             'A monkey is playing drums.',\n",
    "             'A cheetah is running behind its prey.']\n",
    "\n",
    "# Each sentence is encoded as a 1-D vector with 78 columns\n",
    "sentence_embeddings = model.encode(sentences)\n",
    "\n",
    "print('Sample BERT embedding vector - length', len(sentence_embeddings[0]))\n",
    "\n",
    "for sent in sentence_embeddings:\n",
    "    sample = db.convertDocument(sent, {\"hello\": \"world\"}) #convert to aquilaDB object\n",
    "    print(db.addDocuments([sample]))                      # document added\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QLZaLMDuGhEh"
   },
   "source": [
    "## Perform Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search Results\n",
      "result []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json \n",
    "\n",
    "query = 'Nobody has sane thoughts' #@param {type: 'string'}\n",
    "\n",
    "queries = [query]\n",
    "query_embeddings = model.encode(queries)\n",
    "\n",
    "# Find the closest 3 sentences of the corpus for each query sentence based on cosine similarity\n",
    "k = 3 #@param {type: \"number\"}\n",
    "\n",
    "print(\"Semantic Search Results\")\n",
    "\n",
    "q_matrix = db.convertMatrix(query_embeddings[0])\n",
    "result    = db.getNearest(q_matrix, k)\n",
    "\n",
    "print(\"result\", json.loads(result.documents))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SiameseBERT-SemanticSearch.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
